{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Script: train_model.py\n",
    "Descrição:\n",
    "  - Coleta dados históricos da Petrobras (PETR3.SA) usando yfinance\n",
    "  - Prepara dados para previsão multi-step (prever 10 dias à frente)\n",
    "  - Cria e treina um modelo LSTM com saída de 10 neurônios\n",
    "  - Salva o modelo e o scaler para uso na inferência\n",
    "\"\"\"\n",
    "\n",
    "# Importações necessárias\n",
    "import yfinance as yf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import pickle\n",
    "\n",
    "# Bibliotecas de Machine Learning / Deep Learning\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dropout, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baixando dados do Yahoo Finance...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho do conjunto de treinamento: (2949, 60, 1)\n",
      "Tamanho do conjunto de teste: (738, 60, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Parâmetros\n",
    "TICKER = \"PETR3.SA\"\n",
    "START_DATE = \"2010-01-01\"\n",
    "END_DATE = datetime.date.today().strftime(\"%Y-%m-%d\")\n",
    "TIME_STEPS = 60         # Número de dias passados usados como entrada\n",
    "FORECAST_HORIZON = 10   # Número de dias futuros que queremos prever\n",
    "\n",
    "# 1. Coleta de dados\n",
    "print(\"Baixando dados do Yahoo Finance...\")\n",
    "df = yf.download(TICKER, start=START_DATE, end=END_DATE)\n",
    "\n",
    "# Vamos usar apenas a coluna 'Close'\n",
    "df = df[['Close']].dropna()  # remove possíveis NaN\n",
    "\n",
    "# 2. Normalização dos dados\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "data_scaled = scaler.fit_transform(df.values)  # shape: (num_samples, 1)\n",
    "\n",
    "# 3. Criação das janelas de 60 dias para prever 10 dias\n",
    "X, y = [], []\n",
    "\n",
    "# Queremos X[i] = 60 valores passados, y[i] = 10 valores seguintes\n",
    "# Precisamos garantir que haja 10 valores após a janela\n",
    "# Logo, o loop vai até len(data_scaled) - TIME_STEPS - FORECAST_HORIZON + 1\n",
    "for i in range(TIME_STEPS, len(data_scaled) - FORECAST_HORIZON + 1):\n",
    "    # A sequência de entrada: do (i - 60) até (i-1)\n",
    "    X.append(data_scaled[i - TIME_STEPS : i, 0])\n",
    "    # A sequência de saída: do (i) até (i + 10 - 1)\n",
    "    y.append(data_scaled[i : i + FORECAST_HORIZON, 0])\n",
    "\n",
    "# Converte para NumPy\n",
    "X = np.array(X)  # shape: (samples, 60)\n",
    "y = np.array(y)  # shape: (samples, 10)\n",
    "\n",
    "# Redimensiona X para (samples, 60, 1) (LSTM espera 3D)\n",
    "X = X.reshape((X.shape[0], X.shape[1], 1))\n",
    "# y fica como (samples, 10)\n",
    "\n",
    "# 4. Divisão em treino e teste\n",
    "split_index = int(0.8 * len(X))\n",
    "X_train, X_test = X[:split_index], X[split_index:]\n",
    "y_train, y_test = y[:split_index], y[split_index:]\n",
    "\n",
    "print(f\"Tamanho do conjunto de treinamento: {X_train.shape}\")\n",
    "print(f\"Tamanho do conjunto de teste: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Close</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2025-02-10</th>\n",
       "      <td>40.029999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-02-11</th>\n",
       "      <td>40.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-02-12</th>\n",
       "      <td>39.320000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-02-13</th>\n",
       "      <td>39.430000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-02-14</th>\n",
       "      <td>40.849998</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Close\n",
       "Date                 \n",
       "2025-02-10  40.029999\n",
       "2025-02-11  40.250000\n",
       "2025-02-12  39.320000\n",
       "2025-02-13  39.430000\n",
       "2025-02-14  40.849998"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Construção do modelo LSTM\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, return_sequences=True, input_shape=(TIME_STEPS, 1)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(50, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(50))\n",
    "model.add(Dropout(0.2))\n",
    "# Camada de saída com 10 neurônios para prever 10 dias à frente\n",
    "model.add(Dense(FORECAST_HORIZON))\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae', 'mape', 'mse', 'accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treinando o modelo LSTM para previsão de 10 dias à frente...\n",
      "Epoch 1/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 57ms/step - accuracy: 0.0861 - loss: 0.0531 - mae: 0.1673 - mape: 27522.6543 - mse: 0.0531 - val_accuracy: 0.1707 - val_loss: 0.0070 - val_mae: 0.0683 - val_mape: 8.5857 - val_mse: 0.0070\n",
      "Epoch 2/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 49ms/step - accuracy: 0.0994 - loss: 0.0095 - mae: 0.0726 - mape: 14549.8848 - mse: 0.0095 - val_accuracy: 0.0718 - val_loss: 0.0117 - val_mae: 0.0928 - val_mape: 11.5189 - val_mse: 0.0117\n",
      "Epoch 3/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 49ms/step - accuracy: 0.0908 - loss: 0.0075 - mae: 0.0639 - mape: 9710.8232 - mse: 0.0075 - val_accuracy: 0.1762 - val_loss: 0.0065 - val_mae: 0.0661 - val_mape: 8.2959 - val_mse: 0.0065\n",
      "Epoch 4/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 52ms/step - accuracy: 0.0941 - loss: 0.0064 - mae: 0.0585 - mape: 14614.5674 - mse: 0.0064 - val_accuracy: 0.1829 - val_loss: 0.0047 - val_mae: 0.0548 - val_mape: 7.0198 - val_mse: 0.0047\n",
      "Epoch 5/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 51ms/step - accuracy: 0.0998 - loss: 0.0054 - mae: 0.0541 - mape: 10458.5537 - mse: 0.0054 - val_accuracy: 0.1382 - val_loss: 0.0058 - val_mae: 0.0621 - val_mape: 7.8211 - val_mse: 0.0058\n",
      "Epoch 6/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 55ms/step - accuracy: 0.0941 - loss: 0.0048 - mae: 0.0508 - mape: 7422.0400 - mse: 0.0048 - val_accuracy: 0.1531 - val_loss: 0.0062 - val_mae: 0.0646 - val_mape: 8.1313 - val_mse: 0.0062\n",
      "Epoch 7/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 50ms/step - accuracy: 0.0917 - loss: 0.0048 - mae: 0.0499 - mape: 4110.8105 - mse: 0.0048 - val_accuracy: 0.1355 - val_loss: 0.0056 - val_mae: 0.0613 - val_mape: 7.7291 - val_mse: 0.0056\n",
      "Epoch 8/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 50ms/step - accuracy: 0.0915 - loss: 0.0040 - mae: 0.0453 - mape: 5626.5225 - mse: 0.0040 - val_accuracy: 0.1301 - val_loss: 0.0040 - val_mae: 0.0505 - val_mape: 6.4911 - val_mse: 0.0040\n",
      "Epoch 9/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 68ms/step - accuracy: 0.1173 - loss: 0.0038 - mae: 0.0446 - mape: 8489.8828 - mse: 0.0038 - val_accuracy: 0.1436 - val_loss: 0.0110 - val_mae: 0.0918 - val_mape: 11.4558 - val_mse: 0.0110\n",
      "Epoch 10/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 100ms/step - accuracy: 0.1217 - loss: 0.0041 - mae: 0.0457 - mape: 2221.1626 - mse: 0.0041 - val_accuracy: 0.0921 - val_loss: 0.0057 - val_mae: 0.0630 - val_mape: 7.9510 - val_mse: 0.0057\n",
      "Epoch 11/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 92ms/step - accuracy: 0.1140 - loss: 0.0037 - mae: 0.0439 - mape: 3828.3745 - mse: 0.0037 - val_accuracy: 0.1206 - val_loss: 0.0043 - val_mae: 0.0535 - val_mape: 6.8224 - val_mse: 0.0043\n",
      "Epoch 12/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 98ms/step - accuracy: 0.1022 - loss: 0.0035 - mae: 0.0422 - mape: 5118.6265 - mse: 0.0035 - val_accuracy: 0.0976 - val_loss: 0.0087 - val_mae: 0.0810 - val_mape: 10.1852 - val_mse: 0.0087\n",
      "Epoch 13/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 95ms/step - accuracy: 0.0977 - loss: 0.0029 - mae: 0.0397 - mape: 9035.8174 - mse: 0.0029 - val_accuracy: 0.1843 - val_loss: 0.0038 - val_mae: 0.0499 - val_mape: 6.4471 - val_mse: 0.0038\n",
      "Epoch 14/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 65ms/step - accuracy: 0.1122 - loss: 0.0029 - mae: 0.0389 - mape: 3795.8403 - mse: 0.0029 - val_accuracy: 0.0840 - val_loss: 0.0059 - val_mae: 0.0645 - val_mape: 8.1753 - val_mse: 0.0059\n",
      "Epoch 15/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 57ms/step - accuracy: 0.0850 - loss: 0.0029 - mae: 0.0390 - mape: 7223.8306 - mse: 0.0029 - val_accuracy: 0.1301 - val_loss: 0.0057 - val_mae: 0.0632 - val_mape: 7.9878 - val_mse: 0.0057\n",
      "Epoch 16/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 54ms/step - accuracy: 0.0975 - loss: 0.0029 - mae: 0.0387 - mape: 8096.2598 - mse: 0.0029 - val_accuracy: 0.1558 - val_loss: 0.0036 - val_mae: 0.0482 - val_mape: 6.2187 - val_mse: 0.0036\n",
      "Epoch 17/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 53ms/step - accuracy: 0.1032 - loss: 0.0025 - mae: 0.0368 - mape: 3366.4966 - mse: 0.0025 - val_accuracy: 0.1599 - val_loss: 0.0030 - val_mae: 0.0431 - val_mape: 5.6536 - val_mse: 0.0030\n",
      "Epoch 18/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 52ms/step - accuracy: 0.0909 - loss: 0.0026 - mae: 0.0372 - mape: 7646.4775 - mse: 0.0026 - val_accuracy: 0.1843 - val_loss: 0.0041 - val_mae: 0.0520 - val_mape: 6.6334 - val_mse: 0.0041\n",
      "Epoch 19/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 51ms/step - accuracy: 0.1118 - loss: 0.0026 - mae: 0.0357 - mape: 8021.6895 - mse: 0.0026 - val_accuracy: 0.0921 - val_loss: 0.0027 - val_mae: 0.0399 - val_mape: 5.3972 - val_mse: 0.0027\n",
      "Epoch 20/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 52ms/step - accuracy: 0.0882 - loss: 0.0023 - mae: 0.0344 - mape: 10972.5195 - mse: 0.0023 - val_accuracy: 0.1843 - val_loss: 0.0027 - val_mae: 0.0403 - val_mape: 5.3455 - val_mse: 0.0027\n",
      "Epoch 21/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 52ms/step - accuracy: 0.0909 - loss: 0.0023 - mae: 0.0343 - mape: 1953.9340 - mse: 0.0023 - val_accuracy: 0.1843 - val_loss: 0.0045 - val_mae: 0.0554 - val_mape: 7.0353 - val_mse: 0.0045\n",
      "Epoch 22/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 51ms/step - accuracy: 0.1081 - loss: 0.0024 - mae: 0.0353 - mape: 14138.4521 - mse: 0.0024 - val_accuracy: 0.1856 - val_loss: 0.0029 - val_mae: 0.0422 - val_mape: 5.5383 - val_mse: 0.0029\n",
      "Epoch 23/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 50ms/step - accuracy: 0.1048 - loss: 0.0023 - mae: 0.0343 - mape: 17160.4297 - mse: 0.0023 - val_accuracy: 0.1816 - val_loss: 0.0032 - val_mae: 0.0448 - val_mape: 5.7967 - val_mse: 0.0032\n",
      "Epoch 24/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 51ms/step - accuracy: 0.1011 - loss: 0.0020 - mae: 0.0320 - mape: 8806.7373 - mse: 0.0020 - val_accuracy: 0.1748 - val_loss: 0.0027 - val_mae: 0.0401 - val_mape: 5.2963 - val_mse: 0.0027\n",
      "Epoch 25/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 51ms/step - accuracy: 0.1150 - loss: 0.0022 - mae: 0.0340 - mape: 10472.0498 - mse: 0.0022 - val_accuracy: 0.1518 - val_loss: 0.0025 - val_mae: 0.0391 - val_mape: 5.1869 - val_mse: 0.0025\n",
      "Epoch 26/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 52ms/step - accuracy: 0.1105 - loss: 0.0021 - mae: 0.0325 - mape: 4337.8354 - mse: 0.0021 - val_accuracy: 0.1843 - val_loss: 0.0035 - val_mae: 0.0472 - val_mape: 6.0473 - val_mse: 0.0035\n",
      "Epoch 27/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 52ms/step - accuracy: 0.1165 - loss: 0.0025 - mae: 0.0355 - mape: 5893.4487 - mse: 0.0025 - val_accuracy: 0.1856 - val_loss: 0.0028 - val_mae: 0.0419 - val_mape: 5.4746 - val_mse: 0.0028\n",
      "Epoch 28/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 53ms/step - accuracy: 0.0954 - loss: 0.0019 - mae: 0.0313 - mape: 4334.2129 - mse: 0.0019 - val_accuracy: 0.1843 - val_loss: 0.0036 - val_mae: 0.0483 - val_mape: 6.1966 - val_mse: 0.0036\n",
      "Epoch 29/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 52ms/step - accuracy: 0.1016 - loss: 0.0024 - mae: 0.0349 - mape: 5886.7075 - mse: 0.0024 - val_accuracy: 0.0610 - val_loss: 0.0025 - val_mae: 0.0389 - val_mape: 5.1382 - val_mse: 0.0025\n",
      "Epoch 30/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 52ms/step - accuracy: 0.1093 - loss: 0.0024 - mae: 0.0333 - mape: 11286.1348 - mse: 0.0024 - val_accuracy: 0.1558 - val_loss: 0.0026 - val_mae: 0.0390 - val_mape: 5.1502 - val_mse: 0.0026\n",
      "Epoch 31/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 51ms/step - accuracy: 0.1005 - loss: 0.0019 - mae: 0.0320 - mape: 19942.7461 - mse: 0.0019 - val_accuracy: 0.1843 - val_loss: 0.0025 - val_mae: 0.0383 - val_mape: 5.0836 - val_mse: 0.0025\n",
      "Epoch 32/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 53ms/step - accuracy: 0.0954 - loss: 0.0019 - mae: 0.0321 - mape: 6023.5635 - mse: 0.0019 - val_accuracy: 0.1843 - val_loss: 0.0030 - val_mae: 0.0431 - val_mape: 5.5837 - val_mse: 0.0030\n",
      "Epoch 33/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 52ms/step - accuracy: 0.1072 - loss: 0.0018 - mae: 0.0311 - mape: 8142.6362 - mse: 0.0018 - val_accuracy: 0.1653 - val_loss: 0.0029 - val_mae: 0.0425 - val_mape: 5.5160 - val_mse: 0.0029\n",
      "Epoch 34/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 56ms/step - accuracy: 0.1121 - loss: 0.0019 - mae: 0.0310 - mape: 7338.3140 - mse: 0.0019 - val_accuracy: 0.0921 - val_loss: 0.0027 - val_mae: 0.0406 - val_mape: 5.3153 - val_mse: 0.0027\n",
      "Epoch 35/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 51ms/step - accuracy: 0.1042 - loss: 0.0019 - mae: 0.0312 - mape: 12950.8457 - mse: 0.0019 - val_accuracy: 0.1829 - val_loss: 0.0033 - val_mae: 0.0464 - val_mape: 5.9509 - val_mse: 0.0033\n",
      "Epoch 36/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 55ms/step - accuracy: 0.0966 - loss: 0.0019 - mae: 0.0314 - mape: 16158.7148 - mse: 0.0019 - val_accuracy: 0.0949 - val_loss: 0.0027 - val_mae: 0.0403 - val_mape: 5.2534 - val_mse: 0.0027\n",
      "Epoch 37/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 53ms/step - accuracy: 0.1091 - loss: 0.0021 - mae: 0.0322 - mape: 9730.6709 - mse: 0.0021 - val_accuracy: 0.1843 - val_loss: 0.0035 - val_mae: 0.0478 - val_mape: 6.1076 - val_mse: 0.0035\n",
      "Epoch 38/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 54ms/step - accuracy: 0.1167 - loss: 0.0019 - mae: 0.0305 - mape: 8916.6816 - mse: 0.0019 - val_accuracy: 0.1504 - val_loss: 0.0025 - val_mae: 0.0392 - val_mape: 5.1400 - val_mse: 0.0025\n",
      "Epoch 39/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 57ms/step - accuracy: 0.1147 - loss: 0.0017 - mae: 0.0302 - mape: 8727.3428 - mse: 0.0017 - val_accuracy: 0.1843 - val_loss: 0.0060 - val_mae: 0.0665 - val_mape: 8.3904 - val_mse: 0.0060\n",
      "Epoch 40/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 58ms/step - accuracy: 0.1046 - loss: 0.0020 - mae: 0.0325 - mape: 8374.5449 - mse: 0.0020 - val_accuracy: 0.1653 - val_loss: 0.0035 - val_mae: 0.0477 - val_mape: 6.1013 - val_mse: 0.0035\n",
      "Epoch 41/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 50ms/step - accuracy: 0.1015 - loss: 0.0017 - mae: 0.0298 - mape: 12027.1084 - mse: 0.0017 - val_accuracy: 0.0583 - val_loss: 0.0030 - val_mae: 0.0441 - val_mape: 5.6854 - val_mse: 0.0030\n"
     ]
    }
   ],
   "source": [
    "# Callback para early stopping\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# 6. Treinamento\n",
    "print(\"Treinando o modelo LSTM para previsão de 10 dias à frente...\")\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks=[early_stop]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avaliando o modelo no conjunto de teste...\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.1861 - loss: 0.0013 - mae: 0.0278 - mape: 6320.9209 - mse: 0.0013\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 7. Avaliação rápida (opcional)\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAvaliando o modelo no conjunto de teste...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m loss, mae, mape, mse \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(X_train, y_train)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoss (MSE) no treinamento: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMAE no treinamento: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmae\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 4)"
     ]
    }
   ],
   "source": [
    "\n",
    "# 7. Avaliação rápida (opcional)\n",
    "print(\"Avaliando o modelo no conjunto de teste...\")\n",
    "loss, mae, mape, mse = model.evaluate(X_train, y_train)\n",
    "print(f\"Loss (MSE) no treinamento: {loss}\")\n",
    "print(f\"MAE no treinamento: {mae}\")\n",
    "print(f\"MAPE no treinamento: {mape}\")\n",
    "print(f\"MSE no treinamento: {mse}\")\n",
    "\n",
    "\n",
    "print('-'*100)\n",
    "loss, mae, mape, mse = model.evaluate(X_test, y_test)\n",
    "print(f\"Loss (MSE) no teste: {loss}\")\n",
    "print(f\"MAE no teste: {mae}\")\n",
    "print(f\"MAPE no teste: {mape}\")\n",
    "print(f\"MSE no teste: {mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tentando ajustar o modelo com keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treinando o modelo LSTM otimizado para previsão de 10 dias à frente...\n",
      "Epoch 1/10\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 93ms/step - loss: 0.7407 - mae: 0.6441 - mape: 272471.4375 - mse: 0.7407 - val_loss: 0.3040 - val_mae: 0.5306 - val_mape: 67.4635 - val_mse: 0.3040\n",
      "Epoch 2/10\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 85ms/step - loss: 0.0626 - mae: 0.1965 - mape: 91728.6953 - mse: 0.0626 - val_loss: 0.2016 - val_mae: 0.4311 - val_mape: 54.3798 - val_mse: 0.2016\n",
      "Epoch 3/10\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 92ms/step - loss: 0.0303 - mae: 0.1355 - mape: 106489.4844 - mse: 0.0303 - val_loss: 0.1306 - val_mae: 0.3445 - val_mape: 43.1797 - val_mse: 0.1306\n",
      "Epoch 4/10\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 88ms/step - loss: 0.0243 - mae: 0.1184 - mape: 87452.9922 - mse: 0.0243 - val_loss: 0.1399 - val_mae: 0.3597 - val_mape: 45.4625 - val_mse: 0.1399\n",
      "Epoch 5/10\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 86ms/step - loss: 0.0224 - mae: 0.1140 - mape: 134544.7031 - mse: 0.0224 - val_loss: 0.0977 - val_mae: 0.2988 - val_mape: 37.6343 - val_mse: 0.0977\n",
      "Epoch 6/10\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 89ms/step - loss: 0.0197 - mae: 0.1058 - mape: 68016.9297 - mse: 0.0197 - val_loss: 0.0669 - val_mae: 0.2466 - val_mape: 31.0069 - val_mse: 0.0669\n",
      "Epoch 7/10\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 87ms/step - loss: 0.0195 - mae: 0.1055 - mape: 106241.0625 - mse: 0.0195 - val_loss: 0.0661 - val_mae: 0.2438 - val_mape: 30.6014 - val_mse: 0.0661\n",
      "Epoch 8/10\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 95ms/step - loss: 0.0181 - mae: 0.1002 - mape: 75838.0156 - mse: 0.0181 - val_loss: 0.0379 - val_mae: 0.1804 - val_mape: 22.4153 - val_mse: 0.0379\n",
      "Epoch 9/10\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 98ms/step - loss: 0.0198 - mae: 0.1083 - mape: 96175.9844 - mse: 0.0198 - val_loss: 0.0434 - val_mae: 0.1959 - val_mape: 24.5530 - val_mse: 0.0434\n",
      "Epoch 10/10\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 101ms/step - loss: 0.0159 - mae: 0.0921 - mape: 77980.0469 - mse: 0.0159 - val_loss: 0.0330 - val_mae: 0.1665 - val_mape: 20.6746 - val_mse: 0.0330\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Exemplo de ajuste de hiperparâmetros:\n",
    "# - Aumentamos o número de neurônios para 64\n",
    "# - Utilizamos dropout e recurrent_dropout diretamente na camada LSTM\n",
    "# - Inserimos camadas de BatchNormalization para estabilizar os gradientes\n",
    "# - Aumentamos o número de épocas para dar mais tempo de treinamento\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, return_sequences=True, \n",
    "               input_shape=(TIME_STEPS, 1),\n",
    "               dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(LSTM(64, return_sequences=True, \n",
    "               dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# Camada de saída para prever FORECAST_HORIZON (por exemplo, 10 dias à frente)\n",
    "model.add(Dense(FORECAST_HORIZON))\n",
    "\n",
    "# Compilação do modelo: removemos 'accuracy' e mantemos métricas relevantes para regressão\n",
    "model.compile(optimizer='adam', \n",
    "              loss='mean_squared_error', \n",
    "              metrics=['mae', 'mape', 'mse'])\n",
    "\n",
    "# Callback para early stopping, monitorando a perda de validação\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "print(\"Treinando o modelo LSTM otimizado para previsão de 10 dias à frente...\")\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=100,  # Aumentamos o número de épocas para melhor convergência\n",
    "    batch_size=32,\n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks=[early_stop]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avaliando o modelo no conjunto de teste...\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - loss: 0.0118 - mae: 0.0939 - mape: 76844.2109 - mse: 0.0118\n",
      "Loss (MSE) no treinamento: 0.01265661045908928\n",
      "MAE no treinamento: 0.09568314254283905\n",
      "MAPE no treinamento: 106913.640625\n",
      "MSE no treinamento: 0.01265661045908928\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0239 - mae: 0.1395 - mape: 18.5129 - mse: 0.0239\n",
      "Loss (MSE) no teste: 0.03304833918809891\n",
      "MAE no teste: 0.16654856503009796\n",
      "MAPE no teste: 20.674577713012695\n",
      "MSE no teste: 0.03304833918809891\n"
     ]
    }
   ],
   "source": [
    "# 7. Avaliação rápida (opcional)\n",
    "print(\"Avaliando o modelo no conjunto de teste...\")\n",
    "loss, mae, mape, mse  = model.evaluate(X_train, y_train)\n",
    "print(f\"Loss (MSE) no treinamento: {loss}\")\n",
    "print(f\"MAE no treinamento: {mae}\")\n",
    "print(f\"MAPE no treinamento: {mape}\")\n",
    "print(f\"MSE no treinamento: {mse}\")\n",
    "\n",
    "\n",
    "print('-'*100)\n",
    "loss, mae, mape, mse  = model.evaluate(X_test, y_test)\n",
    "print(f\"Loss (MSE) no teste: {loss}\")\n",
    "print(f\"MAE no teste: {mae}\")\n",
    "print(f\"MAPE no teste: {mape}\")\n",
    "print(f\"MSE no teste: {mse}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anda otimizando com keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treinando o modelo LSTM otimizado para previsão de 10 dias à frente...\n",
      "Epoch 1/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 151ms/step - loss: 0.6763 - mae: 0.6149 - mape: 414389.2812 - mse: 0.6763 - val_loss: 0.3874 - val_mae: 0.6092 - val_mape: 77.9845 - val_mse: 0.3874 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 174ms/step - loss: 0.0703 - mae: 0.2093 - mape: 116704.3359 - mse: 0.0703 - val_loss: 0.2987 - val_mae: 0.5340 - val_mape: 68.2374 - val_mse: 0.2987 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 174ms/step - loss: 0.0424 - mae: 0.1612 - mape: 96240.3516 - mse: 0.0424 - val_loss: 0.2120 - val_mae: 0.4462 - val_mape: 56.6551 - val_mse: 0.2120 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 170ms/step - loss: 0.0402 - mae: 0.1590 - mape: 112326.1328 - mse: 0.0402 - val_loss: 0.2221 - val_mae: 0.4606 - val_mape: 58.7739 - val_mse: 0.2221 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 191ms/step - loss: 0.0332 - mae: 0.1430 - mape: 122519.4766 - mse: 0.0332 - val_loss: 0.1221 - val_mae: 0.3362 - val_mape: 42.4004 - val_mse: 0.1221 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 199ms/step - loss: 0.0324 - mae: 0.1399 - mape: 135393.0469 - mse: 0.0324 - val_loss: 0.1000 - val_mae: 0.3043 - val_mape: 38.4892 - val_mse: 0.1000 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 209ms/step - loss: 0.0295 - mae: 0.1334 - mape: 101282.7266 - mse: 0.0295 - val_loss: 0.0588 - val_mae: 0.2317 - val_mape: 29.2585 - val_mse: 0.0588 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 246ms/step - loss: 0.0266 - mae: 0.1267 - mape: 69522.5078 - mse: 0.0266 - val_loss: 0.1101 - val_mae: 0.3223 - val_mape: 41.0457 - val_mse: 0.1101 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 207ms/step - loss: 0.0258 - mae: 0.1259 - mape: 123382.4453 - mse: 0.0258 - val_loss: 0.0406 - val_mae: 0.1891 - val_mape: 23.7337 - val_mse: 0.0406 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 164ms/step - loss: 0.0250 - mae: 0.1189 - mape: 118695.2188 - mse: 0.0250 - val_loss: 0.0387 - val_mae: 0.1837 - val_mape: 23.0662 - val_mse: 0.0387 - learning_rate: 0.0010\n",
      "Epoch 11/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 150ms/step - loss: 0.0234 - mae: 0.1195 - mape: 146265.4688 - mse: 0.0234 - val_loss: 0.0283 - val_mae: 0.1526 - val_mape: 18.9711 - val_mse: 0.0283 - learning_rate: 0.0010\n",
      "Epoch 12/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 159ms/step - loss: 0.0232 - mae: 0.1178 - mape: 65859.8281 - mse: 0.0232 - val_loss: 0.0397 - val_mae: 0.1894 - val_mape: 24.0004 - val_mse: 0.0397 - learning_rate: 0.0010\n",
      "Epoch 13/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 151ms/step - loss: 0.0240 - mae: 0.1199 - mape: 74900.6016 - mse: 0.0240 - val_loss: 0.0645 - val_mae: 0.2458 - val_mape: 31.3714 - val_mse: 0.0645 - learning_rate: 0.0010\n",
      "Epoch 14/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 160ms/step - loss: 0.0219 - mae: 0.1137 - mape: 182222.7188 - mse: 0.0219 - val_loss: 0.0682 - val_mae: 0.2525 - val_mape: 32.1788 - val_mse: 0.0682 - learning_rate: 0.0010\n",
      "Epoch 15/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 153ms/step - loss: 0.0217 - mae: 0.1134 - mape: 83944.4141 - mse: 0.0217 - val_loss: 0.0427 - val_mae: 0.1954 - val_mape: 24.5434 - val_mse: 0.0427 - learning_rate: 0.0010\n",
      "Epoch 16/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 168ms/step - loss: 0.0210 - mae: 0.1110 - mape: 96620.8281 - mse: 0.0210 - val_loss: 0.0891 - val_mae: 0.2914 - val_mape: 37.2943 - val_mse: 0.0891 - learning_rate: 0.0010\n",
      "Epoch 17/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 160ms/step - loss: 0.0205 - mae: 0.1093 - mape: 86839.6016 - mse: 0.0205 - val_loss: 0.0787 - val_mae: 0.2728 - val_mape: 34.8039 - val_mse: 0.0787 - learning_rate: 5.0000e-04\n",
      "Epoch 18/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 165ms/step - loss: 0.0190 - mae: 0.1043 - mape: 71360.1641 - mse: 0.0190 - val_loss: 0.0695 - val_mae: 0.2566 - val_mape: 32.8617 - val_mse: 0.0695 - learning_rate: 5.0000e-04\n",
      "Epoch 19/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 143ms/step - loss: 0.0192 - mae: 0.1062 - mape: 195944.0312 - mse: 0.0192 - val_loss: 0.0387 - val_mae: 0.1870 - val_mape: 23.6595 - val_mse: 0.0387 - learning_rate: 5.0000e-04\n",
      "Epoch 20/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 155ms/step - loss: 0.0195 - mae: 0.1057 - mape: 97201.7188 - mse: 0.0195 - val_loss: 0.0333 - val_mae: 0.1725 - val_mape: 21.7692 - val_mse: 0.0333 - learning_rate: 5.0000e-04\n",
      "Epoch 21/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 164ms/step - loss: 0.0196 - mae: 0.1068 - mape: 62515.9766 - mse: 0.0196 - val_loss: 0.0706 - val_mae: 0.2587 - val_mape: 33.1137 - val_mse: 0.0706 - learning_rate: 5.0000e-04\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Primeira camada LSTM com 128 neurônios, dropout e recurrent_dropout de 0.3\n",
    "model.add(LSTM(128, return_sequences=True, input_shape=(TIME_STEPS, 1), \n",
    "               dropout=0.3, recurrent_dropout=0.3))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# Segunda camada LSTM com 128 neurônios\n",
    "model.add(LSTM(128, return_sequences=True, \n",
    "               dropout=0.3, recurrent_dropout=0.3))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# Terceira camada LSTM com 128 neurônios (sem return_sequences)\n",
    "model.add(LSTM(128, dropout=0.3, recurrent_dropout=0.3))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# Camada densa intermediária para refinar o aprendizado\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "# Camada de saída para prever FORECAST_HORIZON (ex: 10 dias)\n",
    "model.add(Dense(FORECAST_HORIZON))\n",
    "\n",
    "# Compilação do modelo - removemos \"accuracy\", pois não é adequada para regressão\n",
    "model.compile(optimizer='adam', \n",
    "              loss='mean_squared_error', \n",
    "              metrics=['mae', 'mape', 'mse'])\n",
    "\n",
    "# Callbacks: EarlyStopping e redução dinâmica da taxa de aprendizado\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "lr_reduce = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
    "\n",
    "print(\"Treinando o modelo LSTM otimizado para previsão de 10 dias à frente...\")\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=100,           # Aumentamos as épocas para permitir melhor convergência\n",
    "    batch_size=32,\n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks=[early_stop, lr_reduce]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avaliando o modelo no conjunto de teste...\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 38ms/step - loss: 0.0100 - mae: 0.0795 - mape: 83180.0000 - mse: 0.0100\n",
      "Loss (MSE) no treinamento: 0.010876620188355446\n",
      "MAE no treinamento: 0.08257120847702026\n",
      "MAPE no treinamento: 115686.140625\n",
      "MSE no treinamento: 0.010876620188355446\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - loss: 0.0220 - mae: 0.1321 - mape: 17.5151 - mse: 0.0220\n",
      "Loss (MSE) no teste: 0.028304418548941612\n",
      "MAE no teste: 0.15256135165691376\n",
      "MAPE no teste: 18.971134185791016\n",
      "MSE no teste: 0.028304418548941612\n"
     ]
    }
   ],
   "source": [
    "# 7. Avaliação rápida (opcional)\n",
    "print(\"Avaliando o modelo no conjunto de teste...\")\n",
    "loss, mae, mape, mse  = model.evaluate(X_train, y_train)\n",
    "print(f\"Loss (MSE) no treinamento: {loss}\")\n",
    "print(f\"MAE no treinamento: {mae}\")\n",
    "print(f\"MAPE no treinamento: {mape}\")\n",
    "print(f\"MSE no treinamento: {mse}\")\n",
    "\n",
    "\n",
    "print('-'*100)\n",
    "loss, mae, mape, mse  = model.evaluate(X_test, y_test)\n",
    "print(f\"Loss (MSE) no teste: {loss}\")\n",
    "print(f\"MAE no teste: {mae}\")\n",
    "print(f\"MAPE no teste: {mape}\")\n",
    "print(f\"MSE no teste: {mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tentando a otimização com o torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_val' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 93\u001b[0m\n\u001b[0;32m     91\u001b[0m X_train \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(X_train, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m     92\u001b[0m y_train \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(y_train, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m---> 93\u001b[0m X_val \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[43mX_val\u001b[49m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m     94\u001b[0m y_val \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(y_val, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m     96\u001b[0m \u001b[38;5;66;03m# Cria DataLoaders para treinamento e validação\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_val' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Definição de hiperparâmetros\n",
    "TIME_STEPS = 30          # Por exemplo, 30 passos de tempo\n",
    "FORECAST_HORIZON = 10    # Previsão de 10 dias à frente\n",
    "INPUT_DIM = 1            # Número de features (por exemplo, apenas o preço)\n",
    "HIDDEN_SIZE = 128\n",
    "DENSE_UNITS = 64\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 100\n",
    "LEARNING_RATE = 0.001\n",
    "EARLY_STOP_PATIENCE = 10\n",
    "\n",
    "# Dispositivo (GPU se disponível)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Modelo LSTM com camadas intermediárias e Batch Normalization\n",
    "class LSTMForecastModel(nn.Module):\n",
    "    def __init__(self, time_steps, forecast_horizon, input_dim=1, hidden_size=128, dense_units=64):\n",
    "        super(LSTMForecastModel, self).__init__()\n",
    "        # Primeira camada LSTM\n",
    "        self.lstm1 = nn.LSTM(input_dim, hidden_size, num_layers=1, batch_first=True)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_size)\n",
    "        \n",
    "        # Segunda camada LSTM\n",
    "        self.lstm2 = nn.LSTM(hidden_size, hidden_size, num_layers=1, batch_first=True)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_size)\n",
    "        \n",
    "        # Terceira camada LSTM\n",
    "        self.lstm3 = nn.LSTM(hidden_size, hidden_size, num_layers=1, batch_first=True)\n",
    "        self.bn3 = nn.BatchNorm1d(hidden_size)\n",
    "        \n",
    "        # Camada densa intermediária\n",
    "        self.fc1 = nn.Linear(hidden_size, dense_units)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "        # Camada de saída para previsão do horizonte desejado\n",
    "        self.fc2 = nn.Linear(dense_units, forecast_horizon)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: (batch, TIME_STEPS, INPUT_DIM)\n",
    "        out, _ = self.lstm1(x)\n",
    "        # Aplicando BatchNorm: precisamos transpor para (batch, features, seq_len)\n",
    "        out = out.transpose(1, 2)\n",
    "        out = self.bn1(out)\n",
    "        out = out.transpose(1, 2)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        out, _ = self.lstm2(out)\n",
    "        out = out.transpose(1, 2)\n",
    "        out = self.bn2(out)\n",
    "        out = out.transpose(1, 2)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        out, _ = self.lstm3(out)\n",
    "        out = out.transpose(1, 2)\n",
    "        out = self.bn3(out)\n",
    "        out = out.transpose(1, 2)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        # Seleciona a saída do último timestep\n",
    "        out = out[:, -1, :]  # (batch, hidden_size)\n",
    "        out = self.fc1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc2(out)   # (batch, forecast_horizon)\n",
    "        return out\n",
    "\n",
    "# Instancia o modelo e o envia para o dispositivo\n",
    "model = LSTMForecastModel(TIME_STEPS, FORECAST_HORIZON, INPUT_DIM, HIDDEN_SIZE, DENSE_UNITS)\n",
    "model.to(device)\n",
    "\n",
    "# Definição da função de perda e do otimizador\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, min_lr=1e-6)\n",
    "\n",
    "# Supondo que você já possua os dados de treinamento e validação como tensores:\n",
    "# X_train, y_train, X_val, y_val\n",
    "# Eles devem ter as formas:\n",
    "# X_train: (num_amostras_train, TIME_STEPS, INPUT_DIM)\n",
    "# y_train: (num_amostras_train, FORECAST_HORIZON)\n",
    "# (o mesmo para os dados de validação)\n",
    "\n",
    "import torch\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val = torch.tensor(y_val, dtype=torch.float32)\n",
    "\n",
    "# Cria DataLoaders para treinamento e validação\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "val_dataset = TensorDataset(X_val, y_val)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Loop de treinamento com early stopping\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        batch_X = batch_X.to(device)\n",
    "        batch_y = batch_y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(batch_X)\n",
    "        loss = criterion(predictions, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item() * batch_X.size(0)\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    \n",
    "    # Validação\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in val_loader:\n",
    "            batch_X = batch_X.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "            predictions = model(batch_X)\n",
    "            loss = criterion(predictions, batch_y)\n",
    "            val_loss += loss.item() * batch_X.size(0)\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    \n",
    "    # Ajusta a taxa de aprendizado com base na perda de validação\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    print(f\"Época {epoch+1}/{EPOCHS} - Treino: {train_loss:.4f} - Validação: {val_loss:.4f}\")\n",
    "    \n",
    "    # Early Stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        best_model_state = model.state_dict()\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= EARLY_STOP_PATIENCE:\n",
    "            print(\"Early stopping acionado.\")\n",
    "            break\n",
    "\n",
    "# Carrega o melhor modelo obtido\n",
    "model.load_state_dict(best_model_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('-'*100)\n",
    "# 8. Salvando o modelo e o scaler\n",
    "print(\"Salvando o modelo em 'lstm_model_petr3.h5'...\")\n",
    "model.save(\"lstm_model_petr3.h5\")\n",
    "\n",
    "print(\"Salvando o scaler em 'scaler.pkl'...\")\n",
    "with open(\"scaler.pkl\", \"wb\") as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "print(\"Treinamento concluído e arquivos salvos com sucesso.\")                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
